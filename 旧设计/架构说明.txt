1. 文档预处理模块
1.1 文档结构分析器
Python
class DocumentStructureAnalyzer:
    """文档结构分析，负责识别文档的层次结构和格式"""

    def analyze_document_type(self, text: str) -> str:
        """
        文档类型判断 - ✅保留（100%准确率）
        识别输入文本是经文、论著还是注释类文本

        流程：
        1. 提取文档特征（开头语、结构标记等）
        2. 使用预训练分类器进行分类
        3. 返回文档类型标签

        使用模型：文档分类器
        准确率：100%
        """

    def detect_chapters(self, text: str) -> List[Chapter]:
        """
        章节识别 - ✅保留
        识别藏文章节标记如ལེའུ་དང་པོ།（第一章）等

        流程：
        1. 使用正则表达式匹配章节标记
        2. 提取章节标题和位置
        3. 构建章节层次结构

        使用模型：规则引擎
        """

    def detect_paragraphs(self, text: str) -> List[Paragraph]:
        """
        段落检测 - ✅保留
        通过空行、缩进等特征识别段落边界

        流程：
        1. 识别空行和缩进模式
        2. 根据藏文段落习惯分段
        3. 保留段落间的逻辑关系

        使用模型：规则引擎
        """

    def identify_special_formats(self, text: str) -> Dict[str, List[TextSegment]]:
        """
        特殊格式识别 - ✅保留
        识别偈颂、引文、注释等特殊格式文本

        流程：
        1. 匹配偈颂格式（七字句、九字句等）
        2. 识别引文标记（《》等）
        3. 检测注释标记和格式
        4. 返回格式类型和位置映射

        使用模型：规则引擎+模式匹配
        """
1.2 智能分句器
Python
class IntelligentSegmenter:
    """智能分句系统，负责将文本切分为翻译单元"""

    def segment_sentences(self, text: str) -> List[str]:
        """
        主分句方法 - ✅保留
        综合多种策略进行句子分割

        流程：
        1. 基于规则的初步分句
        2. TiBERT辅助验证完整性
        3. 优化过长过短句子
        4. 返回优化后的句子列表

        准确率：85-90%
        """

    def rule_based_split(self, text: str) -> List[str]:
        """
        基于规则的分句 - ✅保留
        使用藏文标点符号进行基础分句

        流程：
        1. 识别句号།、双句号།།等句子结束标记
        2. 处理特殊情况（如缩写、数字等）
        3. 避免在引号内分句

        使用模型：规则引擎
        """

    def validate_completeness(self, sentences: List[str]) -> List[str]:
        """
        语义完整性验证 - ✅保留
        确保分句没有破坏语义完整性

        流程：
        1. 使用TiBERT检查每个句子的词性完整性
        2. 验证动词、主语等核心成分
        3. 合并不完整的片段

        使用模型：TiBERT词性标注
        """

    def optimize_sentence_length(self, sentences: List[str]) -> List[str]:
        """
        句子长度优化 - ✅保留
        处理过长和过短的句子

        流程：
        1. 识别过短句子（<20字符）
           - 检查是否可与相邻句子合并
           - 保持语义连贯性
        2. 识别过长句子（>150字符）
           - 寻找合适的分割点（如连词、从句边界）
           - 确保NLLB能有效处理
        3. 返回优化后的句子列表

        目的：提高翻译质量，避免上下文不足或注意力失效
        """
1.3 基础元数据提取器
Python
class BasicMetadataExtractor:
    """简化的文档级元数据提取"""

    def extract_metadata(self, document: str) -> Dict:
        """
        提取基础元数据 - ⚠️简化
        只提取必要的文档级信息

        流程：
        1. 识别文档类型（经文/论著/注释）
        2. 基础主题分类（中观/唯识等）
        3. 不再做时代特征等复杂分析

        调整原因：减少不必要的分析开销
        """

    def identify_doc_type(self, document: str) -> str:
        """
        文档类型识别 - ⚠️简化
        简单的类型分类

        流程：
        1. 基于关键词匹配
        2. 返回基础类型标签

        使用模型：关键词匹配
        """

    def basic_categorization(self, document: str) -> str:
        """
        基础主题分类 - ⚠️简化
        粗粒度的主题分类

        流程：
        1. 提取高频佛教术语
        2. 匹配主题模板
        3. 返回主要类别

        使用模型：关键词统计
        """
2. 智能语言分析模块
2.1 TiBERT分析器（修正版）
Python
class EnhancedTiBERTAnalyzer:
    """TiBERT分析器 - 调整后的版本，移除了错误的语法角色推断"""

    def __init__(self):
        self.tibert = TiBERTModel()
        self.case_detector = CaseMarkerDetector()  # 替换原来的grammar_analyzer
        self.term_extractor = TermExtractor()
        self.honorific_detector = HonorificDetector()

    def analyze_sentence(self, sentence: str) -> Dict:
        """
        句子级综合分析 - ⚠️调整
        基于TiBERT的能力范围进行分析

        流程：
        1. TiBERT基础分析（词性标注）
        2. 术语识别（95%准确率）
        3. 格标记检测（只检测不推断角色）
        4. 敬语检测（28%识别率）

        返回：包含各项分析结果的字典
        """

    def extract_terms(self, tibert_output: Dict) -> List[Term]:
        """
        术语识别 - ✅保留
        识别佛教专业术语

        流程：
        1. 基于词性和上下文识别术语
        2. 匹配术语库验证
        3. 计算术语重要性得分

        准确率：95%
        使用模型：TiBERT + 术语库
        """

    def detect_case_markers(self, tibert_output: Dict) -> List[CaseMarker]:
        """
        格标记检测 - ⚠️调整（原identify_grammar_roles）
        只检测格助词，不推断语法角色

        流程：
        1. 识别所有助词（PART词性）
        2. 判断格助词类型（施事格/与格/属格等）
        3. 记录位置和前接词
        4. 不再推断主语/宾语角色（因为NLLB能处理）

        调整原因：NLLB已能很好理解格助词，无需额外推断
        """

    def detect_honorifics(self, tibert_output: Dict) -> List[Honorific]:
        """
        敬语检测 - ⚠️调整
        识别敬语词汇，只记录不修改

        流程：
        1. 匹配敬语词典
        2. 识别敬语语法标记
        3. 记录到元数据（不插入标记）

        识别率：28%（需后处理增强）
        调整：从预处理标记改为元数据记录
        """

    def calculate_term_density(self, terms: List[Term], text_length: int) -> float:
        """
        术语密度计算 - ✅保留
        评估文本的专业程度

        流程：
        1. 统计术语数量
        2. 计算相对于文本长度的密度
        3. 用于后续翻译参数调整

        使用模型：统计计算
        """
2.2 MT5语义编码器（校正版）
Python
class CorrectedMT5Encoder:
    """MT5编码器，包含复杂度校正和置信度预测"""

    def __init__(self):
        self.mt5 = MT5Model()
        self.dimension_adapter = DimensionAdapter()
        self.complexity_corrector = ComplexityCorrector()
        self.confidence_predictor = ConfidencePredictor()

    def analyze_sentence(self, sentence: str, tibert_embedding: Optional[Tensor] = None) -> Dict:
        """
        句子分析主方法 - ✅保留
        进行语义分析和复杂度评估

        流程：
        1. 如有TiBERT嵌入，进行维度适配（768→1024）
        2. 计算原始复杂度并校正
        3. 分析语义密度
        4. 预测翻译置信度

        返回：包含复杂度、语义信息、置信度的字典
        """

    def compute_corrected_complexity(self, sentence: str) -> Dict[str, float]:
        """
        复杂度评分with校正 - ⚠️调整
        修正MT5对简单句的高估问题

        流程：
        1. MT5计算原始复杂度分数
        2. 基于句子长度进行校正：
           - 短句(<50字): 原始分数 * 0.5
           - 长句(>150字): 原始分数 * 1.1
           - 中等句子: 原始分数 * 0.85
        3. 计算置信度

        调整原因：测试显示MT5对简单句评估偏高（60%准确率）
        """

    def predict_translation_confidence(self, sentence: str, complexity: float, metadata: Dict) -> float:
        """
        翻译置信度预测 - ➕新增
        预测NLLB翻译质量的可能性

        流程：
        1. 综合多个因素：
           - 句子复杂度（权重30%）
           - 术语覆盖率（权重20%）
           - 句子长度（权重10%）
           - MT5语义密度（权重40%）
        2. 加权计算总置信度
        3. 归一化到[0,1]区间

        准确率：75%
        用途：决定后处理策略
        """

    def analyze_semantic_density(self, sentence: str) -> float:
        """
        语义密度分析 - ✅保留
        评估信息密度和概念复杂度

        流程：
        1. 计算注意力分布熵
        2. 分析概念词密度
        3. 评估句子信息量

        使用模型：MT5编码器
        """
2.3 维度适配器（新增）
Python
class DimensionAdapter:
    """TiBERT和MT5之间的维度转换器"""

    def __init__(self):
        self.projection = nn.Linear(768, 1024)
        self.load_pretrained_weights()

    def adapt(self, tibert_output: Tensor) -> Tensor:
        """
        维度适配 - ➕新增
        将TiBERT的768维输出转换为MT5的1024维

        流程：
        1. 接收TiBERT的768维嵌入
        2. 通过预训练的线性投影层转换
        3. 返回1024维嵌入供MT5使用

        目的：解决模型间维度不匹配问题
        """

    def load_pretrained_weights(self):
        """
        加载预训练权重 - ➕新增
        加载维度转换的预训练参数

        流程：
        1. 检查权重文件是否存在
        2. 加载预训练的投影矩阵
        3. 冻结参数避免训练时改变
        """
3. 智能决策模块（新增）
3.1 处理策略决策器
Python
class ProcessingStrategyDecider:
    """基于分析结果决定处理策略的智能决策器"""

    def __init__(self):
        self.complexity_threshold = 0.7
        self.confidence_thresholds = {'high': 0.85, 'medium': 0.6}

    def decide_processing_depth(self, metadata: Dict) -> str:
        """
        决定处理深度 - ➕新增
        基于复杂度和置信度决定处理策略

        流程：
        1. 评估句子复杂度
        2. 检查预测置信度
        3. 决定处理模式：
           - minimal: 置信度>0.85，只处理术语和敬语
           - standard: 置信度0.6-0.85，标准处理
           - deep: 置信度<0.6，全面处理

        返回：处理模式标签
        """

    def should_query_knowledge_base(self, term_density: float, confidence: float) -> bool:
        """
        知识库查询决策 - ➕新增
        决定是否需要查询知识库

        流程：
        1. 如果术语密度>0.1，必须查询
        2. 如果置信度<0.7，建议查询
        3. 简单句可跳过查询

        目的：优化性能，避免不必要的查询
        """

    def determine_postprocess_strategy(self, confidence: float, has_honorifics: bool) -> str:
        """
        后处理策略决定 - ➕新增
        基于各种因素决定后处理强度

        流程：
        1. 高置信度：最小干预
        2. 中置信度：标准处理
        3. 低置信度：深度处理
        4. 有敬语时强制增强处理

        原则：信任NLLB能力，按需增强
        """
4. 知识库查询模块（优化版）
4.1 术语库管理器
Python
class TermDatabaseManager:
    """术语库查询和管理"""

    def query_term(self, term: str, context: Optional[str] = None) -> Optional[TermTranslation]:
        """
        术语查询主方法 - ✅保留
        多策略术语查询

        流程：
        1. 精确匹配查询（哈希表）
        2. 失败则模糊匹配（编辑距离）
        3. 多义词则上下文消歧（MT5相似度）
        4. 更新使用频率统计

        覆盖率：66.7%
        """

    def exact_match(self, term: str) -> Optional[TermTranslation]:
        """
        精确匹配 - ✅保留
        哈希表快速查询

        流程：
        1. 标准化术语形式
        2. 哈希表直接查询
        3. 返回匹配结果

        性能：O(1)
        """

    def fuzzy_match(self, term: str, threshold: float = 0.8) -> List[TermTranslation]:
        """
        模糊匹配 - ✅保留
        处理拼写变体

        流程：
        1. 计算与库中术语的编辑距离
        2. 返回相似度超过阈值的候选
        3. 按相似度排序

        使用算法：Levenshtein距离
        """

    def select_by_context(self, candidates: List[TermTranslation], context: str) -> TermTranslation:
        """
        上下文消歧 - ✅保留
        为多义词选择最合适的翻译

        流程：
        1. 使用MT5计算上下文相似度
        2. 选择语义最匹配的翻译
        3. 记录选择用于学习

        使用模型：MT5语义相似度
        """
4.2 翻译记忆库
Python
class TranslationMemoryManager:
    """翻译记忆库管理，提供历史翻译复用"""

    def __init__(self):
        self.similarity_threshold = 0.5  # 调整后的阈值
        self.memory_index = FaissIndex()

    def search_similar(self, sentence: str) -> List[TranslationMemoryEntry]:
        """
        相似句子搜索 - ⚠️调整
        查找相似的历史翻译

        流程：
        1. 使用MT5编码句子
        2. 在向量索引中搜索
        3. 返回相似度>0.5的结果

        调整：阈值从0.85降到0.5，提高复用率
        """

    def extract_reusable_segments(self, sentence: str) -> List[Segment]:
        """
        可复用片段提取 - ✅保留
        提取可复用的短语和片段

        流程：
        1. N-gram分割（3-7 gram）
        2. 匹配记忆库中的片段
        3. 返回高质量片段

        用途：部分匹配复用
        """
4.3 参数推荐库（新增）
Python
class ParameterRecommender:
    """基于句子特征推荐NLLB参数"""

    def get_translation_params(self, complexity_score: float, metadata: Dict) -> Dict:
        """
        翻译参数推荐 - ➕新增
        基于复杂度动态调整NLLB参数

        流程：
        1. 根据复杂度分段：
           - 低(<0.3): beam_size=3, temperature=1.0
           - 中(0.3-0.7): beam_size=5, temperature=0.9
           - 高(>0.7): beam_size=8, temperature=0.8
        2. 固定repetition_penalty=1.25（最优值）
        3. 调整length_penalty避免过短/过长

        目的：优化不同复杂度句子的翻译质量
        """

    def adjust_for_special_cases(self, params: Dict, metadata: Dict) -> Dict:
        """
        特殊情况参数调整 - ➕新增
        针对特殊文本类型微调参数

        流程：
        1. 偈颂：降低temperature保持格式
        2. 术语密集：提高beam_size
        3. 敬语文本：调整length_penalty

        返回：调整后的参数字典
        """
5. 智能元数据管理器（优化版）
5.1 元数据收集器
Python
class OptimizedMetadataManager:
    """优化的元数据管理，实现按需收集和智能组织"""

    def __init__(self):
        self.sentence_metadata = {}
        self.collection_strategy = CollectionStrategy()

    def collect_analysis(self, sentence_id: str, source: str, analyses: Dict) -> None:
        """
        智能收集分析结果 - ⚠️优化
        基于句子特征决定收集深度

        流程：
        1. 评估句子复杂度
        2. 决定收集策略：
           - 简单句：只收集术语和基础信息
           - 复杂句：收集完整语法信息
        3. 优先存储高价值信息

        优化原因：减少不必要的存储和处理
        """

    def should_collect_detailed_grammar(self, complexity: float, confidence: float) -> bool:
        """
        语法信息收集决策 - ➕新增
        决定是否需要详细语法分析

        流程：
        1. 复杂度>0.7必须收集
        2. 置信度<0.6建议收集
        3. 其他情况跳过

        目的：避免过度分析
        """

    def prioritize_metadata(self, metadata: Dict) -> Dict:
        """
        元数据优先级排序 - ➕新增
        按重要性组织元数据

        优先级顺序：
        1. 术语信息（始终需要）
        2. 敬语信息（NLLB弱项）
        3. 复杂度和置信度
        4. 格标记（仅复杂句）
        5. 其他语法信息

        返回：优先级排序的元数据
        """
6. NLLB翻译引擎（优化版）
6.1 智能翻译执行器
Python
class OptimizedNLLBEngine:
    """优化的NLLB翻译引擎，实现缓存优先和参数自适应"""

    def __init__(self):
        self.nllb = NLLBModel()
        self.cache = TranslationCache(size=10000)  # large_cache配置
        self.context_builder = ContextWindowBuilder()

    def translate_with_context(self, source: str, metadata: Dict) -> TranslationResult:
        """
        智能翻译主方法 - ✅优化
        基于元数据的自适应翻译

        流程：
        1. 缓存检查（91.8%命中率）
        2. 参数设置（基于复杂度）
        3. 上下文窗口构建（2-3句）
        4. 执行NLLB翻译
        5. 缓存结果

        优化：缓存优先，参数自适应
        """

    def build_context_window(self, source: str, metadata: Dict) -> str:
        """
        上下文窗口构建 - ⚠️调整
        构建固定大小的上下文窗口

        流程：
        1. 获取前2-3句作为上下文
        2. 检查上下文相关性
        3. 构建输入格式

        调整：固定2-3句，避免除零错误
        """

    def set_adaptive_parameters(self, metadata: Dict) -> Dict:
        """
        自适应参数设置 - ✅优化
        基于元数据动态设置翻译参数

        流程：
        1. 从参数推荐库获取基础参数
        2. 固定repetition_penalty=1.25
        3. 根据特殊情况微调

        核心参数：
        - num_beams: 3-8（基于复杂度）
        - temperature: 0.8-1.0
        - repetition_penalty: 1.25（固定）
        """
7. 自适应后处理模块
7.1 分级后处理器
Python
class AdaptivePostProcessor:
    """基于置信度的分级后处理系统"""

    def __init__(self):
        self.error_corrector = ErrorCorrector()
        self.repetition_remover = RepetitionRemover()
        self.confidence_calculator = ConfidenceCalculator()
        self.processing_strategies = {
            'minimal': MinimalProcessor(),
            'standard': StandardProcessor(),
            'deep': DeepProcessor()
        }

    def process(self, translation: str, metadata: Dict, nllb_output: NLLBOutput) -> ProcessedResult:
        """
        自适应后处理主方法 - ➕新增
        根据置信度选择处理策略

        流程：
        1. 计算翻译置信度（75%准确率）
        2. 基础修正（100%成功率）
        3. 根据置信度选择策略：
           - >0.85: 最小干预
           - 0.6-0.85: 标准处理
           - <0.6: 深度处理
        4. 记录处理日志

        原则：高置信度少干预，低置信度深处理
        """

    def calculate_confidence(self, nllb_output: NLLBOutput, metadata: Dict) -> float:
        """
        置信度计算 - ➕新增
        综合多因素计算翻译置信度

        因素权重：
        - NLLB内部分数: 40%
        - 句子复杂度: 30%
        - 术语覆盖率: 20%
        - 句子长度: 10%

        准确率：75%
        """
7.2 分级处理策略
Python
class MinimalProcessor:
    """最小干预处理器 - 用于高置信度翻译"""

    def process(self, translation: str, metadata: Dict) -> str:
        """
        最小干预处理 - ➕新增
        只处理必要项目

        流程：
        1. 术语一致性检查（始终需要）
        2. 敬语增强（NLLB弱项）
        3. 基本格式调整

        适用：置信度>0.85的翻译
        """

    def ensure_term_consistency(self, translation: str, terms: List[Term]) -> str:
        """
        术语一致性保证 - ✅必需
        确保术语翻译的一致性

        流程：
        1. 检查术语是否正确翻译
        2. 修正不一致的术语
        3. 保持全文术语统一

        原因：术语准确性至关重要
        """

class StandardProcessor:
    """标准处理器 - 用于中等置信度翻译"""

    def process(self, translation: str, metadata: Dict) -> str:
        """
        标准处理 - ➕新增
        常规处理流程

        流程：
        1. 最小处理的所有步骤
        2. 简单语法检查
        3. 基础语序调整

        适用：置信度0.6-0.85的翻译
        """

    def check_basic_grammar(self, translation: str, metadata: Dict) -> str:
        """
        基础语法检查 - ⚠️轻度
        只检查明显的语法问题

        流程：
        1. 检查主谓一致
        2. 验证基本语序
        3. 修正明显错误

        原则：信任NLLB，少干预
        """

class DeepProcessor:
    """深度处理器 - 用于低置信度翻译"""

    def process(self, translation: str, metadata: Dict) -> str:
        """
        深度处理 - ➕新增
        全面处理和修正

        流程：
        1. 标准处理的所有步骤
        2. 深度语法分析
        3. 复杂语序调整
        4. 标记需人工审核

        适用：置信度<0.6的翻译
        """

    def mark_for_review(self, translation: str, issues: List[str]) -> str:
        """
        人工审核标记 - ➕新增
        标记低置信度翻译供人工审核

        流程：
        1. 添加置信度标记
        2. 列出潜在问题
        3. 提供修正建议

        目的：质量保证
        """
8. 质量控制与反馈模块
8.1 质量监控器
Python
class QualityMonitor:
    """翻译质量监控和统计"""

    def track_processing_metrics(self, result: ProcessedResult) -> None:
        """
        处理指标跟踪 - ➕新增
        记录处理过程的各项指标

        跟踪内容：
        1. 置信度分布
        2. 处理策略使用频率
        3. 术语覆盖率
        4. 错误修正统计

        用途：持续优化系统
        """

    def generate_quality_report(self, batch_results: List[ProcessedResult]) -> QualityReport:
        """
        质量报告生成 - ➕新增
        生成批次翻译的质量报告

        报告内容：
        1. 平均置信度
        2. 问题类型分布
        3. 处理时间统计
        4. 改进建议

        频率：每1000句生成一次
        """
8.2 反馈学习器
Python
class FeedbackLearner:
    """基于用户反馈的系统优化"""

    def learn_from_corrections(self, original: str, corrected: str, metadata: Dict) -> None:
        """
        从人工修正中学习 - ✅重要
        分析人工修正以改进系统

        流程：
        1. 对比分析差异
        2. 提取新术语对
        3. 识别错误模式
        4. 更新规则库

        学习内容：
        - 新术语翻译
        - 错误修正规则
        - 参数优化建议
        """

    def update_confidence_model(self, predictions: List[float], actual_quality: List[float]) -> None:
        """
        置信度模型更新 - ➕新增
        改进置信度预测准确率

        流程：
        1. 收集预测vs实际质量数据
        2. 分析预测偏差
        3. 调整权重参数
        4. 验证改进效果

        目标：将75%准确率提升到85%+
        """
系统集成流程
Python
class TibetanTranslationPipeline:
    """完整的藏文翻译流水线"""

    def translate_document(self, document: str) -> TranslatedDocument:
        """
        文档翻译主流程

        执行步骤：
        1. 文档预处理
           - 文档类型识别（100%准确率）
           - 结构分析和分句（85-90%准确率）
           - 基础元数据提取

        2. 智能语言分析
           - TiBERT分析（术语95%，敬语28%）
           - MT5分析（复杂度+置信度预测）
           - 智能决策（处理深度判断）

        3. 知识库增强
           - 术语查询（66.7%覆盖）
           - 记忆库检索（阈值0.5）
           - 参数推荐（基于复杂度）

        4. NLLB翻译
           - 缓存优先（91.8%命中）
           - 参数自适应
           - 上下文增强

        5. 自适应后处理
           - 置信度计算（75%准确率）
           - 分级处理（最小/标准/深度）
           - 质量保证

        6. 输出生成
           - 格式恢复
           - 质量标记
           - 统计报告
        """